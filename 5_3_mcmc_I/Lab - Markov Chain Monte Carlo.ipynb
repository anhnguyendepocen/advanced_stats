{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Markov chains\n",
    "\n",
    "Markov chains are crucial to Markov Chain Monte Carlo (as you would expect). So, we'll spend a few minutes here practicing with them and looking at a few properties.\n",
    "\n",
    "As noted in the lecture, the state of a Markov chain is dependent on only its most recent previous state. This is made clear by this factorization:\n",
    "\n",
    "$$P(x_t|x_{t-1}, x_{t-2}, ..., x_{0}) = P(x_t | x_{t-1})$$\n",
    "\n",
    "This is known as the \"memorylessness\" property.\n",
    "\n",
    "Additionally, for use in MCMC, our Markov chains need to have the ergodic property. In other words, they must be irreducible, time-homogenous, aperiodic, and have a stationary distribution.\n",
    "\n",
    "(Irreducible: Every state is reachable from every other state.)\n",
    "\n",
    "(Time-homogenous: The transition matrix does not depend on the sampling time step.)\n",
    "\n",
    "(Aperiodic: There are (generally) no constraints on path length between states.)\n",
    "\n",
    "The Markov chain transition matrix must also be a \"stochastic matrix\": all values are nonnegative, and every row sums to one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Implement a predicate function to determine if a given matrix is a stochastic matrix. You may assume the input is a 2-dimensional square `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Given a finite square transition matrix, how could you verify that it is irreducible? Think algorithmically. (Note that I am asking for a plain-English explanation. If you wish, you can implement a function for this but it is not required.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Implement a function that computes one step of a Markov chain. It should take as input a sample vector and a transition matrix, and return a new probability vector. You may assume the inputs are semantically valid for the Markov chain (i.e. that the sample vector sums to one, and that the transition matrix is ergodic).\n",
    "\n",
    "You can begin with this boilerplate:\n",
    "\n",
    "```python\n",
    "def step_markov_chain(sample, transition_matrix):\n",
    "    # do work\n",
    "    return new_sample\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Use your function to find the stationary distribution of the following transition matrix T.\n",
    "\n",
    "Remember that the initial sample (the guess) must be a valid probability assignment (sum to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "T = np.array([[0.2, 0.8], [0.1, 0.9]])\n",
    "\n",
    "x = pass # DO THIS\n",
    "print(x)\n",
    "for i in range(10):\n",
    "    x = step_markov_chain(x, T)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) In your own words, describe what the stationary distribution is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Experiment with three different starting guesses for the first sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Why is the first sample important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Create three different transition matrices and find their stationary distributions. You may need to experiment with your matrices to make sure that the stationary distribution exists. In your own words, explain why your transition matrices are irreducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preliminaries\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement a Metropolis MCMC sampler, then compare it to one in a standard statistical library.\n",
    "\n",
    "First, we generate some data: 100 points from a normal distribution with mean 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.random.normal(loc=1, size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot a histogram of those points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "sns.distplot(data, kde=False, ax=ax)\n",
    "_ = ax.set(title='Histogram of observed data', xlabel='x', ylabel='# observations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose a model. We already know the data generating process is a normal distribution with mean 1 and variance 0. For the sake of argument, we will try to infer the mean.\n",
    "\n",
    "Because we are inferring the mean, the posterior distribution of our model will be our estimate for $\\mu$:\n",
    "\n",
    "$$\\mu \\sim \\mathcal{N}(0, 1)$$\n",
    "\n",
    "$$x | \\mu \\sim \\mathcal{N}(x; \\mu, 1)$$\n",
    "\n",
    "In words, this says:\n",
    "\n",
    "The mean has a prior distribution of $\\mathcal{N}(0, 1)$ (which we chose arbitrarily).\n",
    "\n",
    "The data, conditioned on the (unknown) mean, is distributed according to a normal distribution (which is parameterized by the data, the unknown mean, and a $\\sigma$ of 1).\n",
    "\n",
    "\n",
    "We will now build a Metropolis sampler for inferring one unknown parameter.\n",
    "\n",
    "Recall the general MCMC algorithm:\n",
    "0. Begin with an initial point.\n",
    "1. Create a new proposal point to \"jump\" to.\n",
    "2. Evaluate its likelihood and create an acceptance ratio.\n",
    "3. Accept the proposal according to that acceptance ratio.\n",
    "5. Goto 2.\n",
    "\n",
    "Fill in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make_p is a utility function for making a version of our model with the proposed mu.\n",
    "# using this separates the model from the sampler.\n",
    "def make_p(mu):\n",
    "    return norm(mu, 1)\n",
    "\n",
    "def metropolis_sampler(initial_point, data, p_maker, n_samples, prior, proposal_stddev):\n",
    "    posterior = [initial_point]\n",
    "    \n",
    "    # the result of p_maker should support the 'pdf' function\n",
    "    \n",
    "    n_dims = len(initial_point) # the number of dimensions\n",
    "    current = initial_point\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Metropolis creates proposals from a normal distribution:\n",
    "        proposal = norm.rvs(current, proposal_stddev, n_dims)\n",
    "        \n",
    "        # Compute likelihood by multiplying probabilities of each point:\n",
    "        likelihood_current = p_maker(current).pdf(data).prod()\n",
    "        likelihood_proposal = pass # DO THIS\n",
    "        \n",
    "        # Compute the prior probability of current and proposed value:\n",
    "        prior_current = prior.pdf(current)\n",
    "        prior_proposal = pass # DO THIS\n",
    "        \n",
    "        p_current = likelihood_current * prior_current\n",
    "        p_proposal = pass # DO THIS\n",
    "        \n",
    "        # Accept proposal?\n",
    "        p_accept = pass # DO THIS\n",
    "        \n",
    "        # Usually would include prior probability, which we neglect here for simplicity\n",
    "        accept = np.random.rand() < p_accept\n",
    "        \n",
    "        if accept:\n",
    "            # Update position\n",
    "            current = proposal\n",
    "    \n",
    "        posterior.append(current)\n",
    "    return posterior\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use it to compute the expectation of $\\mu$ in this model with 1.1 as an initial guess:\n",
    "\n",
    "```python\n",
    "metropolis_sampler([1.1], data, make_p, 100, norm(), 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the points 2, -1, 0, 1, 2 as initial guesses and compute the expected means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different numbers of samples. Conjecture a relationship between the initial guess and the convergence rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to find an initial guess that does not seem to converge after 1000 samples. (Note that you may have to avoid values that are too extreme, due to numerical overflow issues. [If you wish, you can update the sampler to operate in log-probability space to fix this.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different proposal standard deviation values. Conjecture a relationship between the stddev and the convergence rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a plot of our traces. This plot shows the value of each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traces = metropolis_sampler([1.1], data, make_p, 1000, norm(), 0.1)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot([a[0] for a in traces])\n",
    "_ = ax.set(xlabel='sample', ylabel='mu');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot traces of three other initial guesses and three other proposal standard deviations. Give explanations for why the plots look the way they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Burn-in\" is a term for the initial samples that we want to discard. Why would we want to discard them? Can you think of a way to determine when we should stop the burn-in process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up, let's use PyMC3 to compare to our handwritten sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "\n",
    "with pm.Model():\n",
    "    mu = pm.Normal('mu', 1, 1)\n",
    "    sigma = 1.\n",
    "    returns = pm.Normal('returns', mu=mu, sd=sigma, observed=data)\n",
    "    \n",
    "    step = pm.Metropolis()\n",
    "    pymc3_traces = pm.sample(15000, step)\n",
    "    \n",
    "assert len(traces) > 1000, 'run the sampler with high n_samples'\n",
    "\n",
    "sns.distplot(pymc3_traces[2000:]['mu'], label='PyMC3 sampler');\n",
    "sns.distplot(traces[500:], label='Hand-written sampler');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify through the above that our sampler matches that of PyMC3's Metropolis sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
